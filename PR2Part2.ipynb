{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "vOCWxF_vEx1q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KG-AP-PMEuxS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load CIFAR-100 data using PyTorch**"
      ],
      "metadata": {
        "id": "nhMSyrvoE_79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94aeb3jeFB91",
        "outputId": "25412371-cb00-4b73-9b4e-cdb733efd88b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:05<00:00, 30024393.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the CNN model using PyTorch**"
      ],
      "metadata": {
        "id": "WfNAhub5FLec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tkZ2clJ2FYqT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instantiate the model, loss function, and optimizer**"
      ],
      "metadata": {
        "id": "OMWAVEBCFkNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "0xbrxvPoFm9-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**"
      ],
      "metadata": {
        "id": "gyHvnQRGLoR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "fKqYr0ZgLr2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d03ecf-c20c-4378-a18e-7c929733f126"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 3.0892\n",
            "Epoch 2 Loss: 2.9071\n",
            "Epoch 3 Loss: 2.0878\n",
            "Epoch 4 Loss: 1.6466\n",
            "Epoch 5 Loss: 1.5771\n",
            "Epoch 6 Loss: 1.5669\n",
            "Epoch 7 Loss: 1.1267\n",
            "Epoch 8 Loss: 1.0695\n",
            "Epoch 9 Loss: 0.5365\n",
            "Epoch 10 Loss: 0.7989\n",
            "Epoch 11 Loss: 0.3818\n",
            "Epoch 12 Loss: 0.2113\n",
            "Epoch 13 Loss: 0.6399\n",
            "Epoch 14 Loss: 0.3267\n",
            "Epoch 15 Loss: 1.0353\n",
            "Epoch 16 Loss: 0.1758\n",
            "Epoch 17 Loss: 0.2120\n",
            "Epoch 18 Loss: 0.1862\n",
            "Epoch 19 Loss: 0.1608\n",
            "Epoch 20 Loss: 0.6179\n",
            "Epoch 21 Loss: 0.1827\n",
            "Epoch 22 Loss: 0.3705\n",
            "Epoch 23 Loss: 0.0343\n",
            "Epoch 24 Loss: 0.0593\n",
            "Epoch 25 Loss: 0.3543\n",
            "Epoch 26 Loss: 0.0225\n",
            "Epoch 27 Loss: 0.3521\n",
            "Epoch 28 Loss: 0.3277\n",
            "Epoch 29 Loss: 0.0817\n",
            "Epoch 30 Loss: 0.0693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the model**"
      ],
      "metadata": {
        "id": "onuP-IZiLyXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Testing Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "id": "Oz5oL_UBTJI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e8a7e6e-14b6-4a20-e023-1ad542327252"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy: 39.09%\n"
          ]
        }
      ]
    }
  ]
}